def L(X, Y, W):
    loss = 0
    N = X.shape[0]*2
    for i, x in enumerate(X):
        loss += (np.dot(x, W) - Y[i])**2
    return 1/N*loss

def L_diff(X, Y, W):
    grad = list()
    for j, w in enumerate(W):
        w = np.mean([(np.dot(X[i], W) - Y[i])*X[i][j] for i in range(X.shape[0])])
        grad.append(w)
    
    return np.array(grad)

def gd(X, Y, n_iter=10**3, rate='inverse', eps=0.0001, converge_esc=True, if_log=False):
    def log(n, cur_loss, w):
        return print('n_iter: %s\n' \
                     'Loss: %s\n' \
                     'w: %s\n' % (n, cur_loss, w))
    
    n = 1
    # index for loss and w lists
    i = 0
    alph = 0.1
    w = np.zeros(X.shape[1])
    loss_list = list()
    w_list = list()
    
    mark = markers.MarkerStyle(marker='o', fillstyle='full')
    plt.xlim(np.min(X), np.max(X))
    plt.ylim(np.min(Y), np.max(Y))
    plt.scatter([x[0] for x in X], [y for y in Y], alpha=0.5, marker=mark)   
    
    while n <= n_iter:
        plt.plot(np.linspace(-3, 3, 7), 
                 np.linspace(-3, 3, 7)*w[0]+bias*w[1], color='red', alpha=alph)
        
        w = w - 1/n*L_diff(X, Y, w)
        w_list.append(w)
        
        cur_loss = L(X, Y, w)
        loss_list.append(cur_loss)
                     
        if converge_esc and n > 1:
            delta = loss_list[i] - loss_list[i-1]
            if abs(delta) <= eps:
                print('Converged:\n' \
                        'n_iter: %s\n' \
                        'Loss: %s\n' \
                        'delta Loss: %s\n' \
                        'w: %s' % (n, cur_loss, round(delta, 4), w))
                return w
        
        if if_log:
            log(n, cur_loss, w)
        
        n += 1
        i += 1
        alph += 0.01
    
    print('Loss: %s\n%s\n' % (L(X, Y, w), w))
    return w
